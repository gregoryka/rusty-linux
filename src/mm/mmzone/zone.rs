pub(crate) struct zone {
    // 	/* Read-mostly fields */

    // 	/* zone watermarks, access with *_wmark_pages(zone) macros */
    // 	unsigned long _watermark[NR_WMARK];
    // 	unsigned long watermark_boost;

    // 	unsigned long nr_reserved_highatomic;
    // 	unsigned long nr_free_highatomic;

    // 	/*
    // 	 * We don't know if the memory that we're going to allocate will be
    // 	 * freeable or/and it will be released eventually, so to avoid totally
    // 	 * wasting several GB of ram we must reserve some of the lower zone
    // 	 * memory (otherwise we risk to run OOM on the lower zones despite
    // 	 * there being tons of freeable ram on the higher zones).  This array is
    // 	 * recalculated at runtime if the sysctl_lowmem_reserve_ratio sysctl
    // 	 * changes.
    // 	 */
    // 	long lowmem_reserve[MAX_NR_ZONES];

    // #ifdef CONFIG_NUMA
    // 	int node;
    // #endif
    // 	struct pglist_data	*zone_pgdat;
    // 	struct per_cpu_pages	__percpu *per_cpu_pageset;
    // 	struct per_cpu_zonestat	__percpu *per_cpu_zonestats;
    // 	/*
    // 	 * the high and batch values are copied to individual pagesets for
    // 	 * faster access
    // 	 */
    // 	int pageset_high_min;
    // 	int pageset_high_max;
    // 	int pageset_batch;

    // #ifndef CONFIG_SPARSEMEM
    // 	/*
    // 	 * Flags for a pageblock_nr_pages block. See pageblock-flags.h.
    // 	 * In SPARSEMEM, this map is stored in struct mem_section
    // 	 */
    // 	unsigned long		*pageblock_flags;
    // #endif /* CONFIG_SPARSEMEM */

    // 	/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
    // 	unsigned long		zone_start_pfn;

    // 	/*
    // 	 * spanned_pages is the total pages spanned by the zone, including
    // 	 * holes, which is calculated as:
    // 	 * 	spanned_pages = zone_end_pfn - zone_start_pfn;
    // 	 *
    // 	 * present_pages is physical pages existing within the zone, which
    // 	 * is calculated as:
    // 	 *	present_pages = spanned_pages - absent_pages(pages in holes);
    // 	 *
    // 	 * present_early_pages is present pages existing within the zone
    // 	 * located on memory available since early boot, excluding hotplugged
    // 	 * memory.
    // 	 *
    // 	 * managed_pages is present pages managed by the buddy system, which
    // 	 * is calculated as (reserved_pages includes pages allocated by the
    // 	 * bootmem allocator):
    // 	 *	managed_pages = present_pages - reserved_pages;
    // 	 *
    // 	 * cma pages is present pages that are assigned for CMA use
    // 	 * (MIGRATE_CMA).
    // 	 *
    // 	 * So present_pages may be used by memory hotplug or memory power
    // 	 * management logic to figure out unmanaged pages by checking
    // 	 * (present_pages - managed_pages). And managed_pages should be used
    // 	 * by page allocator and vm scanner to calculate all kinds of watermarks
    // 	 * and thresholds.
    // 	 *
    // 	 * Locking rules:
    // 	 *
    // 	 * zone_start_pfn and spanned_pages are protected by span_seqlock.
    // 	 * It is a seqlock because it has to be read outside of zone->lock,
    // 	 * and it is done in the main allocator path.  But, it is written
    // 	 * quite infrequently.
    // 	 *
    // 	 * The span_seq lock is declared along with zone->lock because it is
    // 	 * frequently read in proximity to zone->lock.  It's good to
    // 	 * give them a chance of being in the same cacheline.
    // 	 *
    // 	 * Write access to present_pages at runtime should be protected by
    // 	 * mem_hotplug_begin/done(). Any reader who can't tolerant drift of
    // 	 * present_pages should use get_online_mems() to get a stable value.
    // 	 */
    // 	atomic_long_t		managed_pages;
    // 	unsigned long		spanned_pages;
    // 	unsigned long		present_pages;
    // #if defined(CONFIG_MEMORY_HOTPLUG)
    // 	unsigned long		present_early_pages;
    // #endif
    // #ifdef CONFIG_CMA
    // 	unsigned long		cma_pages;
    // #endif

    // 	const char		*name;

    // #ifdef CONFIG_MEMORY_ISOLATION
    // 	/*
    // 	 * Number of isolated pageblock. It is used to solve incorrect
    // 	 * freepage counting problem due to racy retrieving migratetype
    // 	 * of pageblock. Protected by zone->lock.
    // 	 */
    // 	unsigned long		nr_isolate_pageblock;
    // #endif

    // #ifdef CONFIG_MEMORY_HOTPLUG
    // 	/* see spanned/present_pages for more description */
    // 	seqlock_t		span_seqlock;
    // #endif

    // 	int initialized;

    // 	/* Write-intensive fields used from the page allocator */
    // 	CACHELINE_PADDING(_pad1_);

    // 	/* free areas of different sizes */
    // 	struct free_area	free_area[NR_PAGE_ORDERS];

    // #ifdef CONFIG_UNACCEPTED_MEMORY
    // 	/* Pages to be accepted. All pages on the list are MAX_PAGE_ORDER */
    // 	struct list_head	unaccepted_pages;

    // 	/* To be called once the last page in the zone is accepted */
    // 	struct work_struct	unaccepted_cleanup;
    // #endif

    // 	/* zone flags, see below */
    // 	unsigned long		flags;

    // 	/* Primarily protects free_area */
    // 	spinlock_t		lock;

    // 	/* Pages to be freed when next trylock succeeds */
    // 	struct llist_head	trylock_free_pages;

    // 	/* Write-intensive fields used by compaction and vmstats. */
    // 	CACHELINE_PADDING(_pad2_);

    // 	/*
    // 	 * When free pages are below this point, additional steps are taken
    // 	 * when reading the number of free pages to avoid per-cpu counter
    // 	 * drift allowing watermarks to be breached
    // 	 */
    // 	unsigned long percpu_drift_mark;

    // #if defined CONFIG_COMPACTION || defined CONFIG_CMA
    // 	/* pfn where compaction free scanner should start */
    // 	unsigned long		compact_cached_free_pfn;
    // 	/* pfn where compaction migration scanner should start */
    // 	unsigned long		compact_cached_migrate_pfn[ASYNC_AND_SYNC];
    // 	unsigned long		compact_init_migrate_pfn;
    // 	unsigned long		compact_init_free_pfn;
    // #endif

    // #ifdef CONFIG_COMPACTION
    // 	/*
    // 	 * On compaction failure, 1<<compact_defer_shift compactions
    // 	 * are skipped before trying again. The number attempted since
    // 	 * last failure is tracked with compact_considered.
    // 	 * compact_order_failed is the minimum compaction failed order.
    // 	 */
    // 	unsigned int		compact_considered;
    // 	unsigned int		compact_defer_shift;
    // 	int			compact_order_failed;
    // #endif

    // #if defined CONFIG_COMPACTION || defined CONFIG_CMA
    // 	/* Set to true when the PG_migrate_skip bits should be cleared */
    // 	bool			compact_blockskip_flush;
    // #endif

    // 	bool			contiguous;

    // 	CACHELINE_PADDING(_pad3_);
    // 	/* Zone statistics */
    // 	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];
    // 	atomic_long_t		vm_numa_event[NR_VM_NUMA_EVENT_ITEMS];
} // ____cacheline_internodealigned_in_smp;
